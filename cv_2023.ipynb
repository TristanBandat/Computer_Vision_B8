{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1> CV 2023 </h1>"
      ],
      "metadata": {
        "id": "nKq7NrUSe4dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XUbisa1BTZUS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "8d68a87d-4665-4173-8350-1c622757f719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CV2023\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_thljE0e7ij",
        "outputId": "75718a2d-597c-482e-ee75-43c5a38dbe50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   integrator erkl√§ren +Bilder + focal planes\n",
        "*   model impainting\n",
        "* Diagramm vom Model\n",
        "* pre processing\n",
        "*\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YrAfpqVrlTDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Encoder - Decoder Implementation </h1>"
      ],
      "metadata": {
        "id": "3iZ50OtdIvbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the code:\n",
        "\n",
        "\n",
        "*   Download the zip file from WeTransfer\n",
        "*   Either mount drive to Google Colab and upload images there\n",
        "*   or upload to content folder here (will only be for this instance)\n",
        "*   change strings to files accordingly\n"
      ],
      "metadata": {
        "id": "ND3T3ioeJPpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "7-LjTx_OIydS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InpaintingDataset(Dataset):\n",
        "    def __init__(self, input_root, target_root, params_root):\n",
        "        self.input_root = input_root\n",
        "        self.target_root = target_root\n",
        "        self.params_root = params_root\n",
        "        self.input_list = os.listdir(input_root)\n",
        "        self.target_list = os.listdir(target_root)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        input_base_name = os.path.join(self.input_root, self.input_list[idx*6])\n",
        "        target_name = os.path.join(self.target_root, self.target_list[idx])\n",
        "        parts = os.path.basename(target_name).split('_')\n",
        "        params_name = os.path.join(self.params_root, parts[0] + \"_\" + parts[1] + \"_Parameters.txt\")  # Assuming filenames match\n",
        "        # Stack multiple input images with similar names\n",
        "        input_images = []\n",
        "        for suffix in ['0', '0.5', '1', '1.5', '2', '2.5']:\n",
        "            input_name = os.path.join(self.input_root, f\"{parts[0]}_{parts[1]}_{suffix}_integral.png\")\n",
        "            input_image = Image.open(input_name).convert(\"L\")\n",
        "            input_image = transforms.ToTensor()(input_image)\n",
        "            input_images.append(input_image)\n",
        "\n",
        "        # Stack images along a new dimension\n",
        "        stacked_images = torch.cat(input_images, dim=0)\n",
        "        target_image = Image.open(target_name).convert(\"L\")\n",
        "\n",
        "        target_image = transforms.ToTensor()(target_image)\n",
        "\n",
        "        # Load and process additional parameters\n",
        "        with open(params_name, 'r') as file:\n",
        "            params_content = file.read()\n",
        "\n",
        "        # Add additional parameters to the tuple\n",
        "        return stacked_images, target_image, params_content"
      ],
      "metadata": {
        "id": "5TNqbvf8IzFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify your dataset paths for input and target images\n",
        "input_images_path = os.path.join(\"cnn_test_input\", \"x\")\n",
        "target_images_path = os.path.join(\"cnn_test_input\", \"y\")\n",
        "params_path = os.path.join(\"cnn_test_input\", \"params\")\n",
        "\n",
        "# Create datasets\n",
        "dataset = InpaintingDataset(input_root=input_images_path, target_root=target_images_path, params_root=params_path)"
      ],
      "metadata": {
        "id": "Id9gRiN_IzIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "POHNMYQtIzKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# Assuming you have a dataloader named 'dataloader'\n",
        "for batch in dataloader:\n",
        "    input_images, target_images, params = batch\n",
        "    break  # Take the first batch for simplicity"
      ],
      "metadata": {
        "id": "X19XjfBzIzM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tensors to PIL images for visualization\n",
        "print(input_images[0].shape)\n",
        "input_image_pil = TF.to_pil_image(input_images[0])\n",
        "target_image_pil = TF.to_pil_image(target_images[0])\n",
        "\n",
        "# Plotting the images\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Input Image\")\n",
        "plt.imshow(input_image_pil, cmap=\"gray\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Target Image\")\n",
        "plt.imshow(target_image_pil, cmap=\"gray\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KUaHvO9_IzOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class UNetLike(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNetLike, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.down_conv1 = self.conv_block(6, 16)\n",
        "        self.down_conv2 = self.conv_block(16, 32)\n",
        "        self.down_conv3 = self.conv_block(32, 64)\n",
        "        self.down_conv4 = self.conv_block(64, 128)\n",
        "        self.down_conv5 = self.conv_block(128, 256)\n",
        "        #self.down_conv6 = self.conv_block(256, 512)\n",
        "\n",
        "        # Decoder\n",
        "        #self.up_trans_1 = self.up_transpose(512, 256)\n",
        "        #self.up_conv1 = self.conv_block(512, 256)\n",
        "        self.up_trans_2 = self.up_transpose(256, 128)\n",
        "        self.up_conv2 = self.conv_block(256, 128)\n",
        "        self.up_trans_3 = self.up_transpose(128, 64)\n",
        "        self.up_conv3 = self.conv_block(128, 64)\n",
        "        self.up_trans_4 = self.up_transpose(64, 32)\n",
        "        self.up_conv4 = self.conv_block(64, 32)\n",
        "        self.up_trans_5 = self.up_transpose(32, 16)\n",
        "        self.up_conv5 = self.conv_block(32, 16)\n",
        "\n",
        "        # Final output layer\n",
        "        self.out = nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1).to(device),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1).to(device),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def up_transpose(self, in_channels, out_channels):\n",
        "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder path\n",
        "        x1 = self.down_conv1(x)\n",
        "        x2 = self.down_conv2(nn.MaxPool2d(kernel_size=2, stride=2)(x1))\n",
        "        x3 = self.down_conv3(nn.MaxPool2d(kernel_size=2, stride=2)(x2))\n",
        "        x4 = self.down_conv4(nn.MaxPool2d(kernel_size=2, stride=2)(x3))\n",
        "        x5 = self.down_conv5(nn.MaxPool2d(kernel_size=2, stride=2)(x4))\n",
        "        #x6 = self.down_conv6(nn.MaxPool2d(kernel_size=2, stride=2)(x5))\n",
        "\n",
        "        # Decoder path\n",
        "        #x = self.up_trans_1(x6)  # New layer\n",
        "        #x = self.up_conv1(torch.cat([x, x5], 1))\n",
        "\n",
        "        x = self.up_trans_2(x5)\n",
        "        x = self.up_conv2(torch.cat([x, x4], 1))\n",
        "\n",
        "        x = self.up_trans_3(x)\n",
        "        x = self.up_conv3(torch.cat([x, x3], 1))\n",
        "\n",
        "        x = self.up_trans_4(x)\n",
        "        x = self.up_conv4(torch.cat([x, x2], 1))\n",
        "\n",
        "        x = self.up_trans_5(x)\n",
        "        x = self.up_conv5(torch.cat([x, x1], 1))\n",
        "\n",
        "        x = self.out(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cacQHsbLZqh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchgeometry.image import get_gaussian_kernel2d\n",
        "\n",
        "class SSIM(nn.Module):\n",
        "    r\"\"\"Creates a criterion that measures the Structural Similarity (SSIM)\n",
        "    index between each element in the input `x` and target `y`.\n",
        "\n",
        "    The index can be described as:\n",
        "\n",
        "    .. math::\n",
        "\n",
        "      \\text{SSIM}(x, y) = \\frac{(2\\mu_x\\mu_y+c_1)(2\\sigma_{xy}+c_2)}\n",
        "      {(\\mu_x^2+\\mu_y^2+c_1)(\\sigma_x^2+\\sigma_y^2+c_2)}\n",
        "\n",
        "    where:\n",
        "      - :math:`c_1=(k_1 L)^2` and :math:`c_2=(k_2 L)^2` are two variables to\n",
        "        stabilize the division with weak denominator.\n",
        "      - :math:`L` is the dynamic range of the pixel-values (typically this is\n",
        "        :math:`2^{\\#\\text{bits per pixel}}-1`).\n",
        "\n",
        "    the loss, or the Structural dissimilarity (DSSIM) can be finally described\n",
        "    as:\n",
        "\n",
        "    .. math::\n",
        "\n",
        "      \\text{loss}(x, y) = \\frac{1 - \\text{SSIM}(x, y)}{2}\n",
        "\n",
        "    Arguments:\n",
        "        window_size (int): the size of the kernel.\n",
        "        max_val (float): the dynamic range of the images. Default: 1.\n",
        "        reduction (str, optional): Specifies the reduction to apply to the\n",
        "         output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n",
        "         'mean': the sum of the output will be divided by the number of elements\n",
        "         in the output, 'sum': the output will be summed. Default: 'none'.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: the ssim index.\n",
        "\n",
        "    Shape:\n",
        "        - Input: :math:`(B, C, H, W)`\n",
        "        - Target :math:`(B, C, H, W)`\n",
        "        - Output: scale, if reduction is 'none', then :math:`(B, C, H, W)`\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        >>> input1 = torch.rand(1, 4, 5, 5)\n",
        "        >>> input2 = torch.rand(1, 4, 5, 5)\n",
        "        >>> ssim = tgm.losses.SSIM(5, reduction='none')\n",
        "        >>> loss = ssim(input1, input2)  # 1x4x5x5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            window_size: int,\n",
        "            reduction: str = 'none',\n",
        "            max_val: float = 1.0) -> None:\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size: int = window_size\n",
        "        self.max_val: float = max_val\n",
        "        self.reduction: str = reduction\n",
        "\n",
        "        self.window: torch.Tensor = get_gaussian_kernel2d(\n",
        "            (window_size, window_size), (1.5, 1.5))\n",
        "        self.padding: int = self.compute_zero_padding(window_size)\n",
        "\n",
        "        self.C1: float = (0.01 * self.max_val) ** 2\n",
        "        self.C2: float = (0.03 * self.max_val) ** 2\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_zero_padding(kernel_size: int) -> int:\n",
        "        \"\"\"Computes zero padding.\"\"\"\n",
        "        return (kernel_size - 1) // 2\n",
        "\n",
        "    def filter2D(\n",
        "            self,\n",
        "            input: torch.Tensor,\n",
        "            kernel: torch.Tensor,\n",
        "            channel: int) -> torch.Tensor:\n",
        "        return F.conv2d(input, kernel, padding=self.padding, groups=channel)\n",
        "\n",
        "    def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:\n",
        "        if not torch.is_tensor(img1):\n",
        "            raise TypeError(\"Input img1 type is not a torch.Tensor. Got {}\"\n",
        "                            .format(type(img1)))\n",
        "        if not torch.is_tensor(img2):\n",
        "            raise TypeError(\"Input img2 type is not a torch.Tensor. Got {}\"\n",
        "                            .format(type(img2)))\n",
        "        if not len(img1.shape) == 4:\n",
        "            raise ValueError(\"Invalid img1 shape, we expect BxCxHxW. Got: {}\"\n",
        "                             .format(img1.shape))\n",
        "        if not len(img2.shape) == 4:\n",
        "            raise ValueError(\"Invalid img2 shape, we expect BxCxHxW. Got: {}\"\n",
        "                             .format(img2.shape))\n",
        "        if not img1.shape == img2.shape:\n",
        "            raise ValueError(\"img1 and img2 shapes must be the same. Got: {}\"\n",
        "                             .format(img1.shape, img2.shape))\n",
        "        if not img1.device == img2.device:\n",
        "            raise ValueError(\"img1 and img2 must be in the same device. Got: {}\"\n",
        "                             .format(img1.device, img2.device))\n",
        "        if not img1.dtype == img2.dtype:\n",
        "            raise ValueError(\"img1 and img2 must be in the same dtype. Got: {}\"\n",
        "                             .format(img1.dtype, img2.dtype))\n",
        "        # prepare kernel\n",
        "        b, c, h, w = img1.shape\n",
        "        tmp_kernel: torch.Tensor = self.window.to(img1.device).to(img1.dtype)\n",
        "        kernel: torch.Tensor = tmp_kernel.repeat(c, 1, 1, 1)\n",
        "\n",
        "        # compute local mean per channel\n",
        "        mu1: torch.Tensor = self.filter2D(img1, kernel, c)\n",
        "        mu2: torch.Tensor = self.filter2D(img2, kernel, c)\n",
        "\n",
        "        mu1_sq = mu1.pow(2)\n",
        "        mu2_sq = mu2.pow(2)\n",
        "        mu1_mu2 = mu1 * mu2\n",
        "\n",
        "        # compute local sigma per channel\n",
        "        sigma1_sq = self.filter2D(img1 * img1, kernel, c) - mu1_sq\n",
        "        sigma2_sq = self.filter2D(img2 * img2, kernel, c) - mu2_sq\n",
        "        sigma12 = self.filter2D(img1 * img2, kernel, c) - mu1_mu2\n",
        "\n",
        "        ssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \\\n",
        "            ((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))\n",
        "\n",
        "        loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            loss = torch.mean(loss)\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = torch.sum(loss)\n",
        "        elif self.reduction == 'none':\n",
        "            pass\n",
        "        return loss\n",
        "\n",
        "def ssim(\n",
        "        img1: torch.Tensor,\n",
        "        img2: torch.Tensor,\n",
        "        window_size: int,\n",
        "        reduction: str = 'none',\n",
        "        max_val: float = 1.0) -> torch.Tensor:\n",
        "    r\"\"\"Function that measures the Structural Similarity (SSIM) index between\n",
        "    each element in the input `x` and target `y`.\n",
        "\n",
        "    See :class:`torchgeometry.losses.SSIM` for details.\n",
        "    \"\"\"\n",
        "    return SSIM(window_size, reduction, max_val)(img1, img2)"
      ],
      "metadata": {
        "id": "9Vpibz7sin8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SSIMLoss(nn.Module):\n",
        "    def __init__(self, window_size: int = 3, reduction: str = 'mean', max_val: float = 1.0):\n",
        "        super(SSIMLoss, self).__init__()\n",
        "        self.ssim = SSIM(window_size, reduction, max_val)\n",
        "\n",
        "    def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:\n",
        "        return self.ssim(img1, img2)"
      ],
      "metadata": {
        "id": "_qBc6PNei3HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_person_x_y(param):\n",
        "    pose_start_idx = param.find(\"person pose (x,y,z,rot x, rot y, rot z) = \")\n",
        "    if pose_start_idx != -1:\n",
        "        # Check if the substring is found\n",
        "        if pose_start_idx != -1:\n",
        "            # Extract the relevant part of the string\n",
        "            pose_str = param[pose_start_idx:]\n",
        "\n",
        "            # Find the first \"=\" character\n",
        "            equal_sign_idx = pose_str.find(\"=\")\n",
        "\n",
        "            # Extract the values after the \"=\" character\n",
        "            values_str = pose_str[equal_sign_idx+1:].strip()\n",
        "\n",
        "            # Split the values and convert them to integers\n",
        "            values = [int(val) for val in values_str.split()[:2]]\n",
        "\n",
        "            x_value = values[0]\n",
        "            y_value = values[1]\n",
        "    else:\n",
        "        # No person in the picture = no additional loss\n",
        "        x_value = 11\n",
        "        y_value = 11\n",
        "\n",
        "    return x_value, y_value"
      ],
      "metadata": {
        "id": "_q_dwcB-Zx19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SecondaryLoss(nn.Module):\n",
        "    def __init__(self, window_size: int = 3, reduction: str = 'mean', max_val: float = 1.0, scale_factor: int = 10):\n",
        "        super(SecondaryLoss, self).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "        self.ssim = SSIM(window_size, reduction, max_val)\n",
        "\n",
        "    def forward(self, img1: torch.Tensor, img2: torch.Tensor, params) -> torch.Tensor:\n",
        "        # Get the batch size\n",
        "        batch_size = img1.size(0)\n",
        "        x_values, y_values = zip(*[get_person_x_y(p) for p in params])\n",
        "\n",
        "        # Identify indices of images with a person\n",
        "        valid_indices = [i for i, (x_val, y_val) in enumerate(zip(x_values, y_values)) if x_val != 11 and y_val != 11]\n",
        "\n",
        "        cropped_out = []\n",
        "        cropped_gt = []\n",
        "        for i in valid_indices:\n",
        "            # Compute the positions for cropping for each element in the batch\n",
        "            x_pos = 17 * (-x_values[i])\n",
        "            y_pos = 17 * (-y_values[i])\n",
        "\n",
        "            cropped_out.append(TF.crop(img1[i], 256 + x_pos - 25, 256 + y_pos - 25, 50, 50))\n",
        "            cropped_gt.append(TF.crop(img2[i], 256 + x_pos - 25, 256 + y_pos - 25, 50, 50))\n",
        "\n",
        "        # Stack the cropped results to form batches again\n",
        "        cropped_out = torch.stack(cropped_out)\n",
        "        cropped_gt = torch.stack(cropped_gt)\n",
        "\n",
        "        # Calculate SSIM loss on the cropped batches and scale\n",
        "        return self.ssim(cropped_out, cropped_gt) * self.scale_factor"
      ],
      "metadata": {
        "id": "FZ8s5JpUZ0Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Create the model\n",
        "model = UNetLike()\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# SSIM loss function and optimizer\n",
        "ssim_loss_fn = SSIMLoss()\n",
        "secondary_loss_fn = SecondaryLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00015)\n",
        "scheduler = StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, targets, params in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate the SSIM loss\n",
        "        loss = ssim_loss_fn(outputs, targets)\n",
        "        secondary_loss = secondary_loss_fn(outputs, targets, params)\n",
        "        full_loss = loss + secondary_loss #https://stackoverflow.com/questions/53994625/how-can-i-process-multi-loss-in-pytorch\n",
        "        print(loss, secondary_loss)\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        full_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    average_loss = running_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, SSIM Loss: {average_loss}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if average_loss < best_loss:\n",
        "        best_loss = average_loss\n",
        "\n",
        "        # Save the model with the best validation loss\n",
        "        torch.save(model.state_dict(), f\"cnn_test_input/models/model{epoch + 1}.pth\")"
      ],
      "metadata": {
        "id": "9X2nNET3I5BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_path = f\"cnn_test_input/models/model10.pth\"\n",
        "model = UNetLike()\n",
        "state_dict = torch.load(saved_model_path)\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "wxstjabbI5Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Error in this cell, if you have time please look into printing a single image from a forward pass </h3>"
      ],
      "metadata": {
        "id": "WDVMjFvHKAsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "dataloader_single = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for inputs, targets, _ in dataloader_single:\n",
        "    input_images = inputs\n",
        "    target_images = targets\n",
        "    inputs = input_images.to(device)\n",
        "    outputs = model(inputs)\n",
        "    break\n",
        "\n",
        "print(input_images.shape)\n",
        "print(target_images.shape)\n",
        "print(outputs.shape)"
      ],
      "metadata": {
        "id": "sdifRRCSI5GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(input_images)):\n",
        "    input_image_pil = TF.to_pil_image(input_images[i])\n",
        "    target_image_pil = TF.to_pil_image(target_images[i])\n",
        "    output_iamge_pil = TF.to_pil_image(outputs[i])\n",
        "\n",
        "    # Plotting the images\n",
        "    plt.figure(figsize=(12, 4))  # Increase the width to accommodate three images\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.imshow(input_image_pil)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"Target Image\")\n",
        "    plt.imshow(target_image_pil)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"Output\")\n",
        "    plt.imshow(output_iamge_pil)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "q7e29juDI5It"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
