{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1> CV 2023 </h1>"
   ],
   "metadata": {
    "id": "nKq7NrUSe4dI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "XUbisa1BTZUS",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "outputId": "8d68a87d-4665-4173-8350-1c622757f719",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:22:09.906640492Z",
     "start_time": "2024-01-07T11:22:09.868938616Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"CV2023\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_thljE0e7ij",
    "outputId": "75718a2d-597c-482e-ee75-43c5a38dbe50",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:22:09.927242754Z",
     "start_time": "2024-01-07T11:22:09.909617096Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV2023\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "*   integrator erklären +Bilder + focal planes\n",
    "*   model impainting\n",
    "* Diagramm vom Model\n",
    "* pre processing\n",
    "*\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "YrAfpqVrlTDu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1> Encoder - Decoder Implementation </h1>"
   ],
   "metadata": {
    "id": "3iZ50OtdIvbU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To run the code:\n",
    "\n",
    "\n",
    "*   Download the zip file from WeTransfer\n",
    "*   Either mount drive to Google Colab and upload images there\n",
    "*   or upload to content folder here (will only be for this instance)\n",
    "*   change strings to files accordingly\n"
   ],
   "metadata": {
    "id": "ND3T3ioeJPpH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "id": "7-LjTx_OIydS",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:22:09.944892628Z",
     "start_time": "2024-01-07T11:22:09.925811100Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class InpaintingDataset(Dataset):\n",
    "    def __init__(self, input_root, target_root, params_root):\n",
    "        self.input_root = input_root\n",
    "        self.target_root = target_root\n",
    "        self.params_root = params_root\n",
    "        self.input_list = os.listdir(input_root)\n",
    "        self.target_list = os.listdir(target_root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        input_base_name = os.path.join(self.input_root, self.input_list[idx*6])\n",
    "        target_name = os.path.join(self.target_root, self.target_list[idx])\n",
    "        parts = os.path.basename(target_name).split('_')\n",
    "        params_name = os.path.join(self.params_root, parts[0] + \"_\" + parts[1] + \"_Parameters.txt\")  # Assuming filenames match\n",
    "        # Stack multiple input images with similar names\n",
    "        input_images = []\n",
    "        for suffix in ['0', '0.5', '1', '1.5', '2', '2.5']:\n",
    "            input_name = os.path.join(self.input_root, f\"{parts[0]}_{parts[1]}_{suffix}_integral.png\")\n",
    "            input_image = Image.open(input_name).convert(\"L\")\n",
    "            input_image = transforms.ToTensor()(input_image)\n",
    "            input_images.append(input_image)\n",
    "\n",
    "        # Stack images along a new dimension\n",
    "        stacked_images = torch.cat(input_images, dim=0)\n",
    "        target_image = Image.open(target_name).convert(\"L\")\n",
    "\n",
    "        target_image = transforms.ToTensor()(target_image)\n",
    "\n",
    "        # Load and process additional parameters\n",
    "        with open(params_name, 'r') as file:\n",
    "            params_content = file.read()\n",
    "\n",
    "        # Add additional parameters to the tuple\n",
    "        return stacked_images, target_image, params_content"
   ],
   "metadata": {
    "id": "5TNqbvf8IzFv",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:22:09.965109980Z",
     "start_time": "2024-01-07T11:22:09.932238064Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Specify your dataset paths for input and target images\n",
    "input_images_path = os.path.join(\"cnn_test_input\", \"x\")\n",
    "target_images_path = os.path.join(\"cnn_test_input\", \"y\")\n",
    "params_path = os.path.join(\"cnn_test_input\", \"params\")\n",
    "\n",
    "# Create datasets\n",
    "dataset = InpaintingDataset(input_root=input_images_path, target_root=target_images_path, params_root=params_path)"
   ],
   "metadata": {
    "id": "Id9gRiN_IzIb",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:22:09.967915958Z",
     "start_time": "2024-01-07T11:22:09.947763983Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "id": "POHNMYQtIzKl",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:22:09.987668613Z",
     "start_time": "2024-01-07T11:22:09.968493148Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Assuming you have a dataloader named 'dataloader'\n",
    "for batch in dataloader:\n",
    "    input_images, target_images, params = batch\n",
    "    break  # Take the first batch for simplicity"
   ],
   "metadata": {
    "id": "X19XjfBzIzM0",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:22:11.234761587Z",
     "start_time": "2024-01-07T11:22:09.968678020Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert tensors to PIL images for visualization\n",
    "print(input_images[0].shape)\n",
    "input_image_pil = TF.to_pil_image(input_images[0])\n",
    "target_image_pil = TF.to_pil_image(target_images[0])\n",
    "\n",
    "# Plotting the images\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Input Image\")\n",
    "plt.imshow(input_image_pil, cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Target Image\")\n",
    "plt.imshow(target_image_pil, cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "KUaHvO9_IzOy",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:22:11.235798721Z",
     "start_time": "2024-01-07T11:22:11.234622664Z"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 512, 512])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pic should not have > 4 channels. Got 6 channels.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Convert tensors to PIL images for visualization\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(input_images[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m----> 3\u001B[0m input_image_pil \u001B[38;5;241m=\u001B[39m \u001B[43mTF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_pil_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_images\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m target_image_pil \u001B[38;5;241m=\u001B[39m TF\u001B[38;5;241m.\u001B[39mto_pil_image(target_images[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Plotting the images\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/Python3.10/lib/python3.10/site-packages/torchvision/transforms/functional.py:274\u001B[0m, in \u001B[0;36mto_pil_image\u001B[0;34m(pic, mode)\u001B[0m\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;66;03m# check number of channels\u001B[39;00m\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pic\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m4\u001B[39m:\n\u001B[0;32m--> 274\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should not have > 4 channels. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpic\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m channels.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pic, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pic\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m}:\n",
      "\u001B[0;31mValueError\u001B[0m: pic should not have > 4 channels. Got 6 channels."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class UNetLike(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetLike, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.down_conv1 = self.conv_block(6, 16)\n",
    "        self.down_conv2 = self.conv_block(16, 32)\n",
    "        self.down_conv3 = self.conv_block(32, 64)\n",
    "        self.down_conv4 = self.conv_block(64, 128)\n",
    "        self.down_conv5 = self.conv_block(128, 256)\n",
    "        #self.down_conv6 = self.conv_block(256, 512)\n",
    "\n",
    "        # Decoder\n",
    "        #self.up_trans_1 = self.up_transpose(512, 256)\n",
    "        #self.up_conv1 = self.conv_block(512, 256)\n",
    "        self.up_trans_2 = self.up_transpose(256, 128)\n",
    "        self.up_conv2 = self.conv_block(256, 128)\n",
    "        self.up_trans_3 = self.up_transpose(128, 64)\n",
    "        self.up_conv3 = self.conv_block(128, 64)\n",
    "        self.up_trans_4 = self.up_transpose(64, 32)\n",
    "        self.up_conv4 = self.conv_block(64, 32)\n",
    "        self.up_trans_5 = self.up_transpose(32, 16)\n",
    "        self.up_conv5 = self.conv_block(32, 16)\n",
    "\n",
    "        # Final output layer\n",
    "        self.out = nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1).to(device),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1).to(device),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def up_transpose(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        x1 = self.down_conv1(x)\n",
    "        x2 = self.down_conv2(nn.MaxPool2d(kernel_size=2, stride=2)(x1))\n",
    "        x3 = self.down_conv3(nn.MaxPool2d(kernel_size=2, stride=2)(x2))\n",
    "        x4 = self.down_conv4(nn.MaxPool2d(kernel_size=2, stride=2)(x3))\n",
    "        x5 = self.down_conv5(nn.MaxPool2d(kernel_size=2, stride=2)(x4))\n",
    "        #x6 = self.down_conv6(nn.MaxPool2d(kernel_size=2, stride=2)(x5))\n",
    "\n",
    "        # Decoder path\n",
    "        #x = self.up_trans_1(x6)  # New layer\n",
    "        #x = self.up_conv1(torch.cat([x, x5], 1))\n",
    "\n",
    "        x = self.up_trans_2(x5)\n",
    "        x = self.up_conv2(torch.cat([x, x4], 1))\n",
    "\n",
    "        x = self.up_trans_3(x)\n",
    "        x = self.up_conv3(torch.cat([x, x3], 1))\n",
    "\n",
    "        x = self.up_trans_4(x)\n",
    "        x = self.up_conv4(torch.cat([x, x2], 1))\n",
    "\n",
    "        x = self.up_trans_5(x)\n",
    "        x = self.up_conv5(torch.cat([x, x1], 1))\n",
    "\n",
    "        x = self.out(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "cacQHsbLZqh0",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:22:53.271820844Z",
     "start_time": "2024-01-07T11:22:53.268790026Z"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchgeometry.image import get_gaussian_kernel2d\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    r\"\"\"Creates a criterion that measures the Structural Similarity (SSIM)\n",
    "    index between each element in the input `x` and target `y`.\n",
    "\n",
    "    The index can be described as:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "      \\text{SSIM}(x, y) = \\frac{(2\\mu_x\\mu_y+c_1)(2\\sigma_{xy}+c_2)}\n",
    "      {(\\mu_x^2+\\mu_y^2+c_1)(\\sigma_x^2+\\sigma_y^2+c_2)}\n",
    "\n",
    "    where:\n",
    "      - :math:`c_1=(k_1 L)^2` and :math:`c_2=(k_2 L)^2` are two variables to\n",
    "        stabilize the division with weak denominator.\n",
    "      - :math:`L` is the dynamic range of the pixel-values (typically this is\n",
    "        :math:`2^{\\#\\text{bits per pixel}}-1`).\n",
    "\n",
    "    the loss, or the Structural dissimilarity (DSSIM) can be finally described\n",
    "    as:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "      \\text{loss}(x, y) = \\frac{1 - \\text{SSIM}(x, y)}{2}\n",
    "\n",
    "    Arguments:\n",
    "        window_size (int): the size of the kernel.\n",
    "        max_val (float): the dynamic range of the images. Default: 1.\n",
    "        reduction (str, optional): Specifies the reduction to apply to the\n",
    "         output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n",
    "         'mean': the sum of the output will be divided by the number of elements\n",
    "         in the output, 'sum': the output will be summed. Default: 'none'.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: the ssim index.\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(B, C, H, W)`\n",
    "        - Target :math:`(B, C, H, W)`\n",
    "        - Output: scale, if reduction is 'none', then :math:`(B, C, H, W)`\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> input1 = torch.rand(1, 4, 5, 5)\n",
    "        >>> input2 = torch.rand(1, 4, 5, 5)\n",
    "        >>> ssim = tgm.losses.SSIM(5, reduction='none')\n",
    "        >>> loss = ssim(input1, input2)  # 1x4x5x5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            window_size: int,\n",
    "            reduction: str = 'none',\n",
    "            max_val: float = 1.0) -> None:\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size: int = window_size\n",
    "        self.max_val: float = max_val\n",
    "        self.reduction: str = reduction\n",
    "\n",
    "        self.window: torch.Tensor = get_gaussian_kernel2d(\n",
    "            (window_size, window_size), (1.5, 1.5))\n",
    "        self.padding: int = self.compute_zero_padding(window_size)\n",
    "\n",
    "        self.C1: float = (0.01 * self.max_val) ** 2\n",
    "        self.C2: float = (0.03 * self.max_val) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_zero_padding(kernel_size: int) -> int:\n",
    "        \"\"\"Computes zero padding.\"\"\"\n",
    "        return (kernel_size - 1) // 2\n",
    "\n",
    "    def filter2D(\n",
    "            self,\n",
    "            input: torch.Tensor,\n",
    "            kernel: torch.Tensor,\n",
    "            channel: int) -> torch.Tensor:\n",
    "        return F.conv2d(input, kernel, padding=self.padding, groups=channel)\n",
    "\n",
    "    def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:\n",
    "        if not torch.is_tensor(img1):\n",
    "            raise TypeError(\"Input img1 type is not a torch.Tensor. Got {}\"\n",
    "                            .format(type(img1)))\n",
    "        if not torch.is_tensor(img2):\n",
    "            raise TypeError(\"Input img2 type is not a torch.Tensor. Got {}\"\n",
    "                            .format(type(img2)))\n",
    "        if not len(img1.shape) == 4:\n",
    "            raise ValueError(\"Invalid img1 shape, we expect BxCxHxW. Got: {}\"\n",
    "                             .format(img1.shape))\n",
    "        if not len(img2.shape) == 4:\n",
    "            raise ValueError(\"Invalid img2 shape, we expect BxCxHxW. Got: {}\"\n",
    "                             .format(img2.shape))\n",
    "        if not img1.shape == img2.shape:\n",
    "            raise ValueError(\"img1 and img2 shapes must be the same. Got: {}\"\n",
    "                             .format(img1.shape, img2.shape))\n",
    "        if not img1.device == img2.device:\n",
    "            raise ValueError(\"img1 and img2 must be in the same device. Got: {}\"\n",
    "                             .format(img1.device, img2.device))\n",
    "        if not img1.dtype == img2.dtype:\n",
    "            raise ValueError(\"img1 and img2 must be in the same dtype. Got: {}\"\n",
    "                             .format(img1.dtype, img2.dtype))\n",
    "        # prepare kernel\n",
    "        b, c, h, w = img1.shape\n",
    "        tmp_kernel: torch.Tensor = self.window.to(img1.device).to(img1.dtype)\n",
    "        kernel: torch.Tensor = tmp_kernel.repeat(c, 1, 1, 1)\n",
    "\n",
    "        # compute local mean per channel\n",
    "        mu1: torch.Tensor = self.filter2D(img1, kernel, c)\n",
    "        mu2: torch.Tensor = self.filter2D(img2, kernel, c)\n",
    "\n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        # compute local sigma per channel\n",
    "        sigma1_sq = self.filter2D(img1 * img1, kernel, c) - mu1_sq\n",
    "        sigma2_sq = self.filter2D(img2 * img2, kernel, c) - mu2_sq\n",
    "        sigma12 = self.filter2D(img1 * img2, kernel, c) - mu1_mu2\n",
    "\n",
    "        ssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \\\n",
    "            ((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))\n",
    "\n",
    "        loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        elif self.reduction == 'none':\n",
    "            pass\n",
    "        return loss\n",
    "\n",
    "def ssim(\n",
    "        img1: torch.Tensor,\n",
    "        img2: torch.Tensor,\n",
    "        window_size: int,\n",
    "        reduction: str = 'none',\n",
    "        max_val: float = 1.0) -> torch.Tensor:\n",
    "    r\"\"\"Function that measures the Structural Similarity (SSIM) index between\n",
    "    each element in the input `x` and target `y`.\n",
    "\n",
    "    See :class:`torchgeometry.losses.SSIM` for details.\n",
    "    \"\"\"\n",
    "    return SSIM(window_size, reduction, max_val)(img1, img2)"
   ],
   "metadata": {
    "id": "9Vpibz7sin8p",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:26:46.969104254Z",
     "start_time": "2024-01-07T11:26:46.928323437Z"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, window_size: int = 3, reduction: str = 'mean', max_val: float = 1.0):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        self.ssim = SSIM(window_size, reduction, max_val)\n",
    "\n",
    "    def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:\n",
    "        return self.ssim(img1, img2)"
   ],
   "metadata": {
    "id": "_qBc6PNei3HK",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:26:49.471781711Z",
     "start_time": "2024-01-07T11:26:49.467653879Z"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_person_x_y(param):\n",
    "    pose_start_idx = param.find(\"person pose (x,y,z,rot x, rot y, rot z) = \")\n",
    "    if pose_start_idx != -1:\n",
    "        # Check if the substring is found\n",
    "        if pose_start_idx != -1:\n",
    "            # Extract the relevant part of the string\n",
    "            pose_str = param[pose_start_idx:]\n",
    "\n",
    "            # Find the first \"=\" character\n",
    "            equal_sign_idx = pose_str.find(\"=\")\n",
    "\n",
    "            # Extract the values after the \"=\" character\n",
    "            values_str = pose_str[equal_sign_idx+1:].strip()\n",
    "\n",
    "            # Split the values and convert them to integers\n",
    "            values = [int(val) for val in values_str.split()[:2]]\n",
    "\n",
    "            x_value = values[0]\n",
    "            y_value = values[1]\n",
    "    else:\n",
    "        # No person in the picture = no additional loss\n",
    "        x_value = 11\n",
    "        y_value = 11\n",
    "\n",
    "    return x_value, y_value"
   ],
   "metadata": {
    "id": "_q_dwcB-Zx19",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:26:50.390818997Z",
     "start_time": "2024-01-07T11:26:50.388583192Z"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SecondaryLoss(nn.Module):\n",
    "    def __init__(self, window_size: int = 3, reduction: str = 'mean', max_val: float = 1.0, scale_factor: int = 10):\n",
    "        super(SecondaryLoss, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.ssim = SSIM(window_size, reduction, max_val)\n",
    "\n",
    "    def forward(self, img1: torch.Tensor, img2: torch.Tensor, params) -> torch.Tensor:\n",
    "        # Get the batch size\n",
    "        batch_size = img1.size(0)\n",
    "        x_values, y_values = zip(*[get_person_x_y(p) for p in params])\n",
    "\n",
    "        # Identify indices of images with a person\n",
    "        valid_indices = [i for i, (x_val, y_val) in enumerate(zip(x_values, y_values)) if x_val != 11 and y_val != 11]\n",
    "\n",
    "        cropped_out = []\n",
    "        cropped_gt = []\n",
    "        for i in valid_indices:\n",
    "            # Compute the positions for cropping for each element in the batch\n",
    "            x_pos = 17 * (-x_values[i])\n",
    "            y_pos = 17 * (-y_values[i])\n",
    "\n",
    "            cropped_out.append(TF.crop(img1[i], 256 + x_pos - 25, 256 + y_pos - 25, 50, 50))\n",
    "            cropped_gt.append(TF.crop(img2[i], 256 + x_pos - 25, 256 + y_pos - 25, 50, 50))\n",
    "\n",
    "        # Stack the cropped results to form batches again\n",
    "        cropped_out = torch.stack(cropped_out)\n",
    "        cropped_gt = torch.stack(cropped_gt)\n",
    "\n",
    "        # Calculate SSIM loss on the cropped batches and scale\n",
    "        return self.ssim(cropped_out, cropped_gt) * self.scale_factor"
   ],
   "metadata": {
    "id": "FZ8s5JpUZ0Zh",
    "ExecuteTime": {
     "end_time": "2024-01-07T11:26:52.883044107Z",
     "start_time": "2024-01-07T11:26:52.872768925Z"
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Create the model\n",
    "model = UNetLike()\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# SSIM loss function and optimizer\n",
    "ssim_loss_fn = SSIMLoss()\n",
    "secondary_loss_fn = SecondaryLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00015)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets, params in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the SSIM loss\n",
    "        loss = ssim_loss_fn(outputs, targets)\n",
    "        secondary_loss = secondary_loss_fn(outputs, targets, params)\n",
    "        full_loss = loss + secondary_loss #https://stackoverflow.com/questions/53994625/how-can-i-process-multi-loss-in-pytorch\n",
    "        print(loss, secondary_loss)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        full_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    average_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, SSIM Loss: {average_loss}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if average_loss < best_loss:\n",
    "        best_loss = average_loss\n",
    "\n",
    "        # Save the model with the best validation loss\n",
    "        torch.save(model.state_dict(), f\"cnn_test_input/models/model{epoch + 1}.pth\")"
   ],
   "metadata": {
    "id": "9X2nNET3I5BM",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-07T11:26:54.612867480Z"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2372, grad_fn=<MeanBackward0>) tensor(2.6716, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   2%|▏         | 1/62 [01:01<1:02:28, 61.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2061, grad_fn=<MeanBackward0>) tensor(2.6668, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   3%|▎         | 2/62 [02:02<1:01:10, 61.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2532, grad_fn=<MeanBackward0>) tensor(2.9150, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   5%|▍         | 3/62 [03:06<1:01:17, 62.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1803, grad_fn=<MeanBackward0>) tensor(2.2802, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   6%|▋         | 4/62 [04:09<1:00:49, 62.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2175, grad_fn=<MeanBackward0>) tensor(2.3907, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   8%|▊         | 5/62 [05:19<1:02:09, 65.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2453, grad_fn=<MeanBackward0>) tensor(2.7772, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  10%|▉         | 6/62 [06:29<1:02:33, 67.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1954, grad_fn=<MeanBackward0>) tensor(2.4000, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  11%|█▏        | 7/62 [07:35<1:01:06, 66.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2252, grad_fn=<MeanBackward0>) tensor(2.6570, grad_fn=<MulBackward0>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "saved_model_path = f\"cnn_test_input/models/model10.pth\"\n",
    "model = UNetLike()\n",
    "state_dict = torch.load(saved_model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ],
   "metadata": {
    "id": "wxstjabbI5Dw",
    "ExecuteTime": {
     "start_time": "2024-01-07T11:22:11.235652014Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Error in this cell, if you have time please look into printing a single image from a forward pass </h3>"
   ],
   "metadata": {
    "id": "WDVMjFvHKAsQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "dataloader_single = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for inputs, targets, _ in dataloader_single:\n",
    "    input_images = inputs\n",
    "    target_images = targets\n",
    "    inputs = input_images.to(device)\n",
    "    outputs = model(inputs)\n",
    "    break\n",
    "\n",
    "print(input_images.shape)\n",
    "print(target_images.shape)\n",
    "print(outputs.shape)"
   ],
   "metadata": {
    "id": "sdifRRCSI5GB",
    "ExecuteTime": {
     "start_time": "2024-01-07T11:22:11.235692676Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(len(input_images)):\n",
    "    input_image_pil = TF.to_pil_image(input_images[i])\n",
    "    target_image_pil = TF.to_pil_image(target_images[i])\n",
    "    output_iamge_pil = TF.to_pil_image(outputs[i])\n",
    "\n",
    "    # Plotting the images\n",
    "    plt.figure(figsize=(12, 4))  # Increase the width to accommodate three images\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.imshow(input_image_pil)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Target Image\")\n",
    "    plt.imshow(target_image_pil)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Output\")\n",
    "    plt.imshow(output_iamge_pil)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "q7e29juDI5It",
    "ExecuteTime": {
     "start_time": "2024-01-07T11:22:11.235735915Z"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
