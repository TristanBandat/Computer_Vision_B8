{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKq7NrUSe4dI"
   },
   "source": [
    "<h1> CV 2023 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUbisa1BTZUS",
    "outputId": "ffcb0fdb-a074-4e78-effb-b5f57c3ff678",
    "ExecuteTime": {
     "end_time": "2024-01-08T00:33:20.099071900Z",
     "start_time": "2024-01-08T00:33:20.093036700Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAJuDwwqIyf1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "cv_directory = '/content/drive/MyDrive/Colab_Notebooks/CVB/y.tar.gz'\n",
    "cv_target_directory = '/content/drive/MyDrive/Colab_Notebooks/CVB'\n",
    "shutil.unpack_archive(cv_directory, cv_target_directory)\n",
    "\n",
    "cv_directory = '/content/drive/MyDrive/Colab_Notebooks/CVB/params.tar.gz'\n",
    "cv_target_directory = '/content/drive/MyDrive/Colab_Notebooks/CVB'\n",
    "shutil.unpack_archive(cv_directory, cv_target_directory)\n",
    "\n",
    "cv_directory = '/content/drive/MyDrive/Colab_Notebooks/CVB/x.tar.gz'\n",
    "cv_target_directory = '/content/drive/MyDrive/Colab_Notebooks/CVB'\n",
    "shutil.unpack_archive(cv_directory, cv_target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_thljE0e7ij",
    "outputId": "7d48f3a7-8883-4de9-f8b0-04404bab4dc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV2023\n"
     ]
    }
   ],
   "source": [
    "print(\"CV2023\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrAfpqVrlTDu"
   },
   "source": [
    "\n",
    "\n",
    "*   integrator erkl√§ren +Bilder + focal planes\n",
    "*   model impainting\n",
    "* Diagramm vom Model\n",
    "* pre processing\n",
    "*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iZ50OtdIvbU"
   },
   "source": [
    "<h1> Encoder - Decoder Implementation </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ND3T3ioeJPpH"
   },
   "source": [
    "To run the code:\n",
    "\n",
    "\n",
    "*   Download the zip file from WeTransfer\n",
    "*   Either mount drive to Google Colab and upload images there\n",
    "*   or upload to content folder here (will only be for this instance)\n",
    "*   change strings to files accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-LjTx_OIydS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TNqbvf8IzFv"
   },
   "outputs": [],
   "source": [
    "class InpaintingDataset(Dataset):\n",
    "    def __init__(self, input_root, target_root, params_root):\n",
    "        self.input_root = input_root\n",
    "        self.target_root = target_root\n",
    "        self.params_root = params_root\n",
    "        self.input_list = os.listdir(input_root)\n",
    "        self.target_list = os.listdir(target_root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        input_base_name = os.path.join(self.input_root, self.input_list[idx*6])\n",
    "        target_name = os.path.join(self.target_root, self.target_list[idx])\n",
    "        parts = os.path.basename(target_name).split('_')\n",
    "        params_name = os.path.join(self.params_root, parts[0] + \"_\" + parts[1] + \"_Parameters.txt\")  # Assuming filenames match\n",
    "        # Stack multiple input images with similar names\n",
    "        input_images = []\n",
    "        for suffix in ['0', '0.5', '1', '1.5', '2', '2.5']:\n",
    "            input_name = os.path.join(self.input_root, f\"{parts[0]}_{parts[1]}_{suffix}_integral.png\")\n",
    "            input_image = Image.open(input_name).convert(\"L\")\n",
    "            input_image = transforms.ToTensor()(input_image)\n",
    "            input_images.append(input_image)\n",
    "\n",
    "        # Stack images along a new dimension\n",
    "        stacked_images = torch.cat(input_images, dim=0)\n",
    "        target_image = Image.open(target_name).convert(\"L\")\n",
    "\n",
    "        target_image = transforms.ToTensor()(target_image)\n",
    "\n",
    "        # Load and process additional parameters\n",
    "        with open(params_name, 'r') as file:\n",
    "            params_content = file.read()\n",
    "\n",
    "        # Add additional parameters to the tuple\n",
    "        return stacked_images, target_image, params_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_p9PWqsIG5W"
   },
   "outputs": [],
   "source": [
    "# This splitting assumes that you are working offline\n",
    "\n",
    "def reset_folder(p):\n",
    "    if os.path.exists(p):\n",
    "        shutil.rmtree(p)\n",
    "    os.makedirs(p)\n",
    "\n",
    "def split_dataset(path, train_size, seed):\n",
    "    \"\"\"Takes the current folder setup:\n",
    "    -notebook.ipynb\n",
    "    -data\n",
    "        -params\n",
    "        -x\n",
    "        -y\n",
    "        -models\n",
    "    and splits the data into a training set and a validation set with the following directory format\n",
    "    -notebook.ipynb\n",
    "    -data\n",
    "        -params\n",
    "        -x\n",
    "        -y\n",
    "        -models\n",
    "        -train\n",
    "            -x\n",
    "            -y\n",
    "            -params\n",
    "        -val\n",
    "            -x\n",
    "            -y\n",
    "            -params\n",
    "\n",
    "    the function is random with a seed as specified in seed with the split being a value from 0 - 1 for how big the training dataset should be\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    x_images_list = os.listdir(os.path.join(path, \"x\"))\n",
    "    y_images_list = os.listdir(os.path.join(path, \"y\"))\n",
    "    params_list = os.listdir(os.path.join(path, \"params\"))\n",
    "\n",
    "    im_numbers = [s.split('_')[1] for s in y_images_list]\n",
    "    split_index = int(len(im_numbers) * train_size)\n",
    "\n",
    "    np.random.shuffle(im_numbers)\n",
    "    train_image_num = im_numbers[:split_index]\n",
    "    test_image_num = im_numbers[split_index:]\n",
    "\n",
    "    train_path = os.path.join(path, \"train\")\n",
    "    train_x_path = os.path.join(train_path, 'x')\n",
    "    train_y_path = os.path.join(train_path, 'y')\n",
    "    train_p_path = os.path.join(train_path, 'params')\n",
    "\n",
    "    val_path = os.path.join(path, \"val\")\n",
    "    val_x_path = os.path.join(val_path, 'x')\n",
    "    val_y_path = os.path.join(val_path, 'y')\n",
    "    val_p_path = os.path.join(val_path, 'params')\n",
    "\n",
    "    # checks if the folders exist, and if not creates them\n",
    "    reset_folder(train_x_path)\n",
    "    reset_folder(train_y_path)\n",
    "    reset_folder(train_p_path)\n",
    "    reset_folder(val_x_path)\n",
    "    reset_folder(val_y_path)\n",
    "    reset_folder(val_p_path)\n",
    "\n",
    "    x_image_errors = []\n",
    "    for image in tqdm(x_images_list, desc=\"x images\"):\n",
    "        num = image.split('_')[1]\n",
    "        image = os.path.join(os.path.join(path, \"x\"), image)\n",
    "        if num in train_image_num:\n",
    "            shutil.copy(image, train_x_path)\n",
    "        elif num in test_image_num:\n",
    "            shutil.copy(image, val_x_path)\n",
    "        else:\n",
    "            x_image_errors.append(image)\n",
    "\n",
    "    y_image_errors = []\n",
    "    for image in tqdm(y_images_list, desc='y images'):\n",
    "        num = image.split('_')[1]\n",
    "        image = os.path.join(os.path.join(path, \"y\"), image)\n",
    "        if num in train_image_num:\n",
    "            shutil.copy(image, train_y_path)\n",
    "        elif num in test_image_num:\n",
    "            shutil.copy(image, val_y_path)\n",
    "        else:\n",
    "            y_image_errors.append(image)\n",
    "\n",
    "    param_errors = []\n",
    "    for par in tqdm(params_list, desc='Parameters'):\n",
    "        num = par.split('_')[1]\n",
    "        par = os.path.join(os.path.join(path, \"params\"), par)\n",
    "        if num in train_image_num:\n",
    "            shutil.copy(par, train_p_path)\n",
    "        elif num in test_image_num:\n",
    "            shutil.copy(par, val_p_path)\n",
    "        else:\n",
    "            param_errors.append(par)\n",
    "    # this return is only for problem finding purposes\n",
    "    #return x_image_errors, y_image_errors, param_errors, train_image_num, test_image_num\n",
    "\n",
    "split_dataset(\"cnn_test_input\", 0.8, 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Id9gRiN_IzIb"
   },
   "outputs": [],
   "source": [
    "# Specify your dataset paths for input and target images for both training and validation\n",
    "data_path = 'cnn_test_input'\n",
    "train_path = os.path.join(data_path, \"train\")\n",
    "input_images_path_train = os.path.join(train_path, 'x')\n",
    "target_images_path_train = os.path.join(train_path, 'y')\n",
    "params_path_train = os.path.join(train_path, 'params')\n",
    "\n",
    "val_path = os.path.join(data_path, \"val\")\n",
    "input_images_path_val = os.path.join(val_path, 'x')\n",
    "target_images_path_val = os.path.join(val_path, 'y')\n",
    "params_path_val = os.path.join(val_path, 'params')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = InpaintingDataset(input_root=input_images_path_train, target_root=target_images_path_train,params_root=params_path_train)\n",
    "val_dataset = InpaintingDataset(input_root=input_images_path_val, target_root=target_images_path_val,params_root=params_path_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POHNMYQtIzKl"
   },
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X19XjfBzIzM0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Assuming you have a dataloader named 'dataloader'\n",
    "for batch in dataloader:\n",
    "    input_images, target_images, params = batch\n",
    "    break  # Take the first batch for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUaHvO9_IzOy"
   },
   "outputs": [],
   "source": [
    "# Convert tensors to PIL images for visualization\n",
    "print(input_images[0].shape)\n",
    "print(target_images[0].shape)\n",
    "target_image_pil = TF.to_pil_image(target_images[0])\n",
    "\n",
    "# Plotting the images\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.title(\"Input Image: 0\")\n",
    "plt.imshow(TF.to_pil_image(input_images[0][0]), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.title(\"Input Image: Focal Length 0.5\")\n",
    "plt.imshow(TF.to_pil_image(input_images[0][1]), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.title(\"Input Image: Focal Length 1\")\n",
    "plt.imshow(TF.to_pil_image(input_images[0][2]), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.title(\"Input Image: Focal Length 1.5\")\n",
    "plt.imshow(TF.to_pil_image(input_images[0][3]), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.title(\"Input Image: Focal Length 2\")\n",
    "plt.imshow(TF.to_pil_image(input_images[0][4]), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.title(\"Input Image: Focal Length 2.5\")\n",
    "plt.imshow(TF.to_pil_image(input_images[0][5]), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(3, 3, 7)\n",
    "plt.title(\"Target Image\")\n",
    "plt.imshow(TF.to_pil_image(target_images[0]), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cacQHsbLZqh0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class UNetLike(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetLike, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.down_conv1 = self.conv_block(6, 16)\n",
    "        self.down_conv2 = self.conv_block(16, 32)\n",
    "        self.down_conv3 = self.conv_block(32, 64)\n",
    "        self.down_conv4 = self.conv_block(64, 128)\n",
    "        self.down_conv5 = self.conv_block(128, 256)\n",
    "        #self.down_conv6 = self.conv_block(256, 512)\n",
    "\n",
    "        # Decoder\n",
    "        #self.up_trans_1 = self.up_transpose(512, 256)\n",
    "        #self.up_conv1 = self.conv_block(512, 256)\n",
    "        self.up_trans_2 = self.up_transpose(256, 128)\n",
    "        self.up_conv2 = self.conv_block(256, 128)\n",
    "        self.up_trans_3 = self.up_transpose(128, 64)\n",
    "        self.up_conv3 = self.conv_block(128, 64)\n",
    "        self.up_trans_4 = self.up_transpose(64, 32)\n",
    "        self.up_conv4 = self.conv_block(64, 32)\n",
    "        self.up_trans_5 = self.up_transpose(32, 16)\n",
    "        self.up_conv5 = self.conv_block(32, 16)\n",
    "\n",
    "        # Final output layer\n",
    "        self.out = nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1).to(device),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1).to(device),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def up_transpose(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        x1 = self.down_conv1(x)\n",
    "        x2 = self.down_conv2(nn.MaxPool2d(kernel_size=2, stride=2)(x1))\n",
    "        x3 = self.down_conv3(nn.MaxPool2d(kernel_size=2, stride=2)(x2))\n",
    "        x4 = self.down_conv4(nn.MaxPool2d(kernel_size=2, stride=2)(x3))\n",
    "        x5 = self.down_conv5(nn.MaxPool2d(kernel_size=2, stride=2)(x4))\n",
    "        #x6 = self.down_conv6(nn.MaxPool2d(kernel_size=2, stride=2)(x5))\n",
    "\n",
    "        # Decoder path\n",
    "        #x = self.up_trans_1(x6)  # New layer\n",
    "        #x = self.up_conv1(torch.cat([x, x5], 1))\n",
    "\n",
    "        x = self.up_trans_2(x5)\n",
    "        x = self.up_conv2(torch.cat([x, x4], 1))\n",
    "\n",
    "        x = self.up_trans_3(x)\n",
    "        x = self.up_conv3(torch.cat([x, x3], 1))\n",
    "\n",
    "        x = self.up_trans_4(x)\n",
    "        x = self.up_conv4(torch.cat([x, x2], 1))\n",
    "\n",
    "        x = self.up_trans_5(x)\n",
    "        x = self.up_conv5(torch.cat([x, x1], 1))\n",
    "\n",
    "        x = self.out(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Vpibz7sin8p"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchgeometry.image import get_gaussian_kernel2d\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    r\"\"\"Creates a criterion that measures the Structural Similarity (SSIM)\n",
    "    index between each element in the input `x` and target `y`.\n",
    "\n",
    "    The index can be described as:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "      \\text{SSIM}(x, y) = \\frac{(2\\mu_x\\mu_y+c_1)(2\\sigma_{xy}+c_2)}\n",
    "      {(\\mu_x^2+\\mu_y^2+c_1)(\\sigma_x^2+\\sigma_y^2+c_2)}\n",
    "\n",
    "    where:\n",
    "      - :math:`c_1=(k_1 L)^2` and :math:`c_2=(k_2 L)^2` are two variables to\n",
    "        stabilize the division with weak denominator.\n",
    "      - :math:`L` is the dynamic range of the pixel-values (typically this is\n",
    "        :math:`2^{\\#\\text{bits per pixel}}-1`).\n",
    "\n",
    "    the loss, or the Structural dissimilarity (DSSIM) can be finally described\n",
    "    as:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "      \\text{loss}(x, y) = \\frac{1 - \\text{SSIM}(x, y)}{2}\n",
    "\n",
    "    Arguments:\n",
    "        window_size (int): the size of the kernel.\n",
    "        max_val (float): the dynamic range of the images. Default: 1.\n",
    "        reduction (str, optional): Specifies the reduction to apply to the\n",
    "         output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n",
    "         'mean': the sum of the output will be divided by the number of elements\n",
    "         in the output, 'sum': the output will be summed. Default: 'none'.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: the ssim index.\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(B, C, H, W)`\n",
    "        - Target :math:`(B, C, H, W)`\n",
    "        - Output: scale, if reduction is 'none', then :math:`(B, C, H, W)`\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> input1 = torch.rand(1, 4, 5, 5)\n",
    "        >>> input2 = torch.rand(1, 4, 5, 5)\n",
    "        >>> ssim = tgm.losses.SSIM(5, reduction='none')\n",
    "        >>> loss = ssim(input1, input2)  # 1x4x5x5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            window_size: int,\n",
    "            reduction: str = 'none',\n",
    "            max_val: float = 1.0) -> None:\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size: int = window_size\n",
    "        self.max_val: float = max_val\n",
    "        self.reduction: str = reduction\n",
    "\n",
    "        self.window: torch.Tensor = get_gaussian_kernel2d(\n",
    "            (window_size, window_size), (1.5, 1.5))\n",
    "        self.padding: int = self.compute_zero_padding(window_size)\n",
    "\n",
    "        self.C1: float = (0.01 * self.max_val) ** 2\n",
    "        self.C2: float = (0.03 * self.max_val) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_zero_padding(kernel_size: int) -> int:\n",
    "        \"\"\"Computes zero padding.\"\"\"\n",
    "        return (kernel_size - 1) // 2\n",
    "\n",
    "    def filter2D(\n",
    "            self,\n",
    "            input: torch.Tensor,\n",
    "            kernel: torch.Tensor,\n",
    "            channel: int) -> torch.Tensor:\n",
    "        return F.conv2d(input, kernel, padding=self.padding, groups=channel)\n",
    "\n",
    "    def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:\n",
    "        if not torch.is_tensor(img1):\n",
    "            raise TypeError(\"Input img1 type is not a torch.Tensor. Got {}\"\n",
    "                            .format(type(img1)))\n",
    "        if not torch.is_tensor(img2):\n",
    "            raise TypeError(\"Input img2 type is not a torch.Tensor. Got {}\"\n",
    "                            .format(type(img2)))\n",
    "        if not len(img1.shape) == 4:\n",
    "            raise ValueError(\"Invalid img1 shape, we expect BxCxHxW. Got: {}\"\n",
    "                             .format(img1.shape))\n",
    "        if not len(img2.shape) == 4:\n",
    "            raise ValueError(\"Invalid img2 shape, we expect BxCxHxW. Got: {}\"\n",
    "                             .format(img2.shape))\n",
    "        if not img1.shape == img2.shape:\n",
    "            raise ValueError(\"img1 and img2 shapes must be the same. Got: {}\"\n",
    "                             .format(img1.shape, img2.shape))\n",
    "        if not img1.device == img2.device:\n",
    "            raise ValueError(\"img1 and img2 must be in the same device. Got: {}\"\n",
    "                             .format(img1.device, img2.device))\n",
    "        if not img1.dtype == img2.dtype:\n",
    "            raise ValueError(\"img1 and img2 must be in the same dtype. Got: {}\"\n",
    "                             .format(img1.dtype, img2.dtype))\n",
    "        # prepare kernel\n",
    "        b, c, h, w = img1.shape\n",
    "        tmp_kernel: torch.Tensor = self.window.to(img1.device).to(img1.dtype)\n",
    "        kernel: torch.Tensor = tmp_kernel.repeat(c, 1, 1, 1)\n",
    "\n",
    "        # compute local mean per channel\n",
    "        mu1: torch.Tensor = self.filter2D(img1, kernel, c)\n",
    "        mu2: torch.Tensor = self.filter2D(img2, kernel, c)\n",
    "\n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        # compute local sigma per channel\n",
    "        sigma1_sq = self.filter2D(img1 * img1, kernel, c) - mu1_sq\n",
    "        sigma2_sq = self.filter2D(img2 * img2, kernel, c) - mu2_sq\n",
    "        sigma12 = self.filter2D(img1 * img2, kernel, c) - mu1_mu2\n",
    "\n",
    "        ssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / \\\n",
    "            ((mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2))\n",
    "\n",
    "        loss = torch.clamp(1. - ssim_map, min=0, max=1) / 2.\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = torch.sum(loss)\n",
    "        elif self.reduction == 'none':\n",
    "            pass\n",
    "        return loss\n",
    "\n",
    "def ssim(\n",
    "        img1: torch.Tensor,\n",
    "        img2: torch.Tensor,\n",
    "        window_size: int,\n",
    "        reduction: str = 'none',\n",
    "        max_val: float = 1.0) -> torch.Tensor:\n",
    "    r\"\"\"Function that measures the Structural Similarity (SSIM) index between\n",
    "    each element in the input `x` and target `y`.\n",
    "\n",
    "    See :class:`torchgeometry.losses.SSIM` for details.\n",
    "    \"\"\"\n",
    "    return SSIM(window_size, reduction, max_val)(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qBc6PNei3HK"
   },
   "outputs": [],
   "source": [
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, window_size: int = 3, reduction: str = 'mean', max_val: float = 1.0):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        self.ssim = SSIM(window_size, reduction, max_val)\n",
    "\n",
    "    def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:\n",
    "        return self.ssim(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_q_dwcB-Zx19"
   },
   "outputs": [],
   "source": [
    "def get_person_x_y(param):\n",
    "    pose_start_idx = param.find(\"person pose (x,y,z,rot x, rot y, rot z) = \")\n",
    "    if pose_start_idx != -1:\n",
    "        # Check if the substring is found\n",
    "        if pose_start_idx != -1:\n",
    "            # Extract the relevant part of the string\n",
    "            pose_str = param[pose_start_idx:]\n",
    "\n",
    "            # Find the first \"=\" character\n",
    "            equal_sign_idx = pose_str.find(\"=\")\n",
    "\n",
    "            # Extract the values after the \"=\" character\n",
    "            values_str = pose_str[equal_sign_idx+1:].strip()\n",
    "\n",
    "            # Split the values and convert them to integers\n",
    "            values = [int(val) for val in values_str.split()[:2]]\n",
    "\n",
    "            x_value = values[0]\n",
    "            y_value = values[1]\n",
    "    else:\n",
    "        # No person in the picture = no additional loss\n",
    "        x_value = 11\n",
    "        y_value = 11\n",
    "\n",
    "    return x_value, y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ8s5JpUZ0Zh"
   },
   "outputs": [],
   "source": [
    "class SecondaryLoss(nn.Module):\n",
    "    def __init__(self, window_size: int = 3, reduction: str = 'mean', max_val: float = 1.0, scale_factor: int = 10):\n",
    "        super(SecondaryLoss, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.ssim = SSIM(window_size, reduction, max_val)\n",
    "\n",
    "    def forward(self, img1: torch.Tensor, img2: torch.Tensor, params) -> torch.Tensor:\n",
    "        # Get the batch size\n",
    "        batch_size = img1.size(0)\n",
    "        x_values, y_values = zip(*[get_person_x_y(p) for p in params])\n",
    "\n",
    "        # Identify indices of images with a person\n",
    "        valid_indices = [i for i, (x_val, y_val) in enumerate(zip(x_values, y_values)) if x_val != 11 and y_val != 11]\n",
    "\n",
    "        cropped_out = []\n",
    "        cropped_gt = []\n",
    "        for i in valid_indices:\n",
    "            # Compute the positions for cropping for each element in the batch\n",
    "            x_pos = 17 * (-x_values[i])\n",
    "            y_pos = 17 * (-y_values[i])\n",
    "\n",
    "            cropped_out.append(TF.crop(img1[i], 256 + x_pos - 25, 256 + y_pos - 25, 50, 50))\n",
    "            cropped_gt.append(TF.crop(img2[i], 256 + x_pos - 25, 256 + y_pos - 25, 50, 50))\n",
    "\n",
    "        # Stack the cropped results to form batches again\n",
    "        cropped_out = torch.stack(cropped_out)\n",
    "        cropped_gt = torch.stack(cropped_gt)\n",
    "\n",
    "        # Calculate SSIM loss on the cropped batches and scale\n",
    "        return self.ssim(cropped_out, cropped_gt) * self.scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9X2nNET3I5BM"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Create the model\n",
    "model = UNetLike()\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# SSIM loss function and optimizer\n",
    "ssim_loss_fn = SSIMLoss()\n",
    "secondary_loss_fn = SecondaryLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00015)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 4\n",
    "best_loss = float('inf')\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    running_loss_train = 0.0\n",
    "    running_loss_val = 0.0\n",
    "\n",
    "    for inputs, targets, params in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs} Training\"):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the SSIM loss\n",
    "        loss = ssim_loss_fn(outputs, targets)\n",
    "        secondary_loss = secondary_loss_fn(outputs, targets, params)\n",
    "        full_loss = loss + secondary_loss #https://stackoverflow.com/questions/53994625/how-can-i-process-multi-loss-in-pytorch\n",
    "\n",
    "        #print(f'Epoch {epoch + 1}  SSIM Loss: {loss}  Secondary Loss {secondary_loss}')\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        full_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss_train += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    for inputs, targets, params in tqdm(val_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs} Validation\"):\n",
    "        with torch.no_grad():\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # Calculate the SSIM loss\n",
    "            loss = ssim_loss_fn(outputs, targets)\n",
    "            #secondary_loss = secondary_loss_fn(outputs, targets, params)\n",
    "            #full_loss = loss + secondary_loss #https://stackoverflow.com/questions/53994625/how-can-i-process-multi-loss-in-pytorch\n",
    "\n",
    "            running_loss_val += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    average_loss_train = running_loss_train / len(train_dataloader)\n",
    "    average_loss_val = running_loss_val / len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_loss_train}  Validation Loss {average_loss_val}\")\n",
    "\n",
    "    loss_train.append(average_loss_train)\n",
    "    loss_val.append(average_loss_val)\n",
    "\n",
    "    if average_loss_val < best_loss:\n",
    "        best_loss = average_loss_val\n",
    "\n",
    "        # Save the model with the best validation loss\n",
    "        torch.save(model.state_dict(), f\"cnn_test_input/models/model{epoch + 1}.pth\")\n",
    "\n",
    "epoch_x = np.arange(num_epochs) + 1\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(epoch_x, loss_train, marker = 'o', label = \"Training Loss\")\n",
    "plt.plot(epoch_x, loss_val, marker = 'o', label = \"Validation Loss\")\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxstjabbI5Dw"
   },
   "outputs": [],
   "source": [
    "saved_model_path = f\"cnn_test_input/models/model10.pth\"\n",
    "model = UNetLike()\n",
    "state_dict = torch.load(saved_model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDVMjFvHKAsQ"
   },
   "source": [
    "<h3> Error in this cell, if you have time please look into printing a single image from a forward pass </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdifRRCSI5GB"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataloader_single = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for inputs, targets, _ in dataloader_single:\n",
    "    input_images = inputs\n",
    "    target_images = targets\n",
    "    inputs = input_images.to(device)\n",
    "    outputs = model(inputs)\n",
    "    break\n",
    "\n",
    "print(input_images.shape)\n",
    "print(target_images.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7e29juDI5It"
   },
   "outputs": [],
   "source": [
    "for i in range(len(input_images)):\n",
    "    input_image_pil = TF.to_pil_image(input_images[i])\n",
    "    target_image_pil = TF.to_pil_image(target_images[i])\n",
    "    output_iamge_pil = TF.to_pil_image(outputs[i])\n",
    "\n",
    "    # Plotting the images\n",
    "    plt.figure(figsize=(12, 4))  # Increase the width to accommodate three images\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.imshow(input_image_pil)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Target Image\")\n",
    "    plt.imshow(target_image_pil)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Output\")\n",
    "    plt.imshow(output_iamge_pil)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tG18INUQIrYl"
   },
   "outputs": [],
   "source": [
    "for i in range(len(input_images)):\n",
    "    target_image_pil = TF.to_pil_image(target_images[i])\n",
    "    output_iamge_pil = TF.to_pil_image(outputs[i])\n",
    "    print(f'idx: {i}')\n",
    "    # Plotting the images\n",
    "    plt.figure(figsize=(6, 4))  # Increase the width to accommodate three images\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Target Image\")\n",
    "    plt.imshow(target_image_pil, cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Output\")\n",
    "    plt.imshow(output_iamge_pil, cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "labiThCUIr8T"
   },
   "outputs": [],
   "source": [
    "# Convert tensors to PIL images for visualization\n",
    "def get_images(input_im, target_im, out_im, idx):\n",
    "\n",
    "    # Plotting the images\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(3, 3, 1)\n",
    "    plt.title(\"Input Image: 0\")\n",
    "    plt.imshow(TF.to_pil_image(input_images[idx][0]), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 2)\n",
    "    plt.title(\"Input Image: Focal Length 0.5\")\n",
    "    plt.imshow(TF.to_pil_image(input_images[idx][1]), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 3)\n",
    "    plt.title(\"Input Image: Focal Length 1\")\n",
    "    plt.imshow(TF.to_pil_image(input_images[idx][2]), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 4)\n",
    "    plt.title(\"Input Image: Focal Length 1.5\")\n",
    "    plt.imshow(TF.to_pil_image(input_images[idx][3]), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 5)\n",
    "    plt.title(\"Input Image: Focal Length 2\")\n",
    "    plt.imshow(TF.to_pil_image(input_images[idx][4]), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 6)\n",
    "    plt.title(\"Input Image: Focal Length 2.5\")\n",
    "    plt.imshow(TF.to_pil_image(input_images[idx][5]), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 7)\n",
    "    plt.title(\"Target Image\")\n",
    "    plt.imshow(TF.to_pil_image(target_images[idx]), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 8)\n",
    "    plt.title(\"Output Image\")\n",
    "    plt.imshow(TF.to_pil_image(out_im[idx]), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "get_images(input_images, target_images, outputs, 18)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
